{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sktime.forecasting.model_selection import temporal_train_test_split\n",
    "from sktime.utils.plotting import plot_series\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "fix_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/Users/mungeonhui/git/AI_model/dacon/2023전력사용량예측AI경진대회/open'\n",
    "train_csv = os.path.join(DATA_DIR, 'train.csv')\n",
    "test_csv = os.path.join(DATA_DIR, 'test.csv')\n",
    "building_csv = os.path.join(DATA_DIR, 'building_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv(train_csv)\n",
    "test_set = pd.read_csv(test_csv)\n",
    "building_info = pd.read_csv(building_csv)\n",
    "\n",
    "train_df = pd.merge(train_set, building_info, left_on='건물번호', right_on='건물번호')\n",
    "test_df = pd.merge(test_set, building_info, left_on='건물번호', right_on='건물번호')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((204000,), (204000, 15))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature, label 나누기\n",
    "train_label = train_df['전력소비량(kWh)']\n",
    "train_feature = train_df.drop(columns=['전력소비량(kWh)'])\n",
    "\n",
    "train_label.shape, train_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing time transformer\n",
      "initialising text transformer\n",
      "Initializing Mean Imputer\n",
      "Initializing Value Imputer\n",
      "Initializing Get Driven variable estimator\n",
      "initializing drop field\n"
     ]
    }
   ],
   "source": [
    "class GetTimeData(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"initializing time transformer\")\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        X_['month'] = X_['일시'].apply(lambda x : int(x[4:6]))\n",
    "        X_['day'] = X_['일시'].apply(lambda x : int(x[6:8]))\n",
    "        X_['time'] = X_['일시'].apply(lambda x : int(x[9:11]))\n",
    "        X_ = X_.drop(columns=['일시'])\n",
    "        return X_\n",
    "\n",
    "\n",
    "class TextImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, ):\n",
    "        print(\"initialising text transformer\")\n",
    "        self.cols = [\"태양광용량(kW)\", \"ESS저장용량(kWh)\", \"PCS용량(kW)\"]\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        \n",
    "        for col in self.cols:\n",
    "            X_[col] = X_[col].replace('-', 0).astype(\"float64\")\n",
    "        return X_\n",
    "\n",
    "class MeanImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,):\n",
    "        print(\"Initializing Mean Imputer\")\n",
    "        self.imputer = SimpleImputer()\n",
    "        self.cols = ['풍속(m/s)', '습도(%)']\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.imputer.fit(X[self.cols])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        X_[self.cols] = self.imputer.transform(X_[self.cols])\n",
    "        return X_\n",
    "\n",
    "class ValueImputer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"Initializing Value Imputer\")\n",
    "        self.cols = ['강수량(mm)']\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X.copy()\n",
    "        X_[self.cols] = X_[self.cols].fillna(0)\n",
    "        return X_\n",
    "\n",
    "class GetDrivenVar(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"Initializing Get Driven variable estimator\")\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X_ = X\n",
    "        X_['THI'] = 9 / 5 * X_['기온(C)'] - 0.55   \\\n",
    "                    * (1 - X_['습도(%)'] / 100)    \\\n",
    "                    * (9 / 5 * X_['습도(%)'] - 26) \\\n",
    "                    + 32\n",
    "                    \n",
    "        return X_\n",
    "\n",
    "class DropField(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        print(\"initializing drop field\")\n",
    "        self.cols = [\"num_date_time\", \"건물번호\", \"일조(hr)\", \"일사(MJ/m2)\"]\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        columns = X.columns\n",
    "        cols = [col for col in self.cols if col in columns]\n",
    "        X_ = X.copy()\n",
    "        X_ = X_.drop(columns=cols)\n",
    "        return X_\n",
    "      \n",
    "      \n",
    "scale_cols = [\n",
    "    '풍속(m/s)', '습도(%)',\n",
    "    '강수량(mm)', '기온(C)',\n",
    "    '연면적(m2)', '냉방면적(m2)',\n",
    "    '태양광용량(kW)', 'ESS저장용량(kWh)',\n",
    "    'PCS용량(kW)', 'month', 'time', 'day', 'THI'\n",
    "]\n",
    "    \n",
    "column_transformer = make_column_transformer(\n",
    "    (StandardScaler(), scale_cols),\n",
    "    (OneHotEncoder(), ['건물유형']), \n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('time_spliter', GetTimeData()),\n",
    "    ('text_imputer', TextImputer()),\n",
    "    ('mean_imputer', MeanImputer()),\n",
    "    ('value_imputer', ValueImputer()),\n",
    "    ('driven_variable', GetDrivenVar()),\n",
    "    ('drop_field', DropField()), \n",
    "    ('column_transformer', column_transformer),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((204000, 25), (16800, 25))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed = pipeline.fit_transform(train_feature)\n",
    "transformed_test = pipeline.fit_transform(test_df)\n",
    "\n",
    "transformed.shape, transformed_test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(features, labels, val_hour):\n",
    "    if val_hour == 0:\n",
    "        return labels, None, features, None\n",
    "    else:\n",
    "        y_train, y_valid, x_train, x_valid = temporal_train_test_split(y=labels, X=features, test_size=val_hour)\n",
    "        return y_train, y_valid, x_train, x_valid        \n",
    "\n",
    "def SMAPE(y_true, y_pred):\n",
    "    return np.mean((np.abs(y_true - y_pred)) / (np.abs(y_true) + np.abs(y_pred))) * 100\n",
    "\n",
    "def weighted_mse(alpha=1):\n",
    "    def weighted_mse_fixed(label, pred):\n",
    "        residual = (label - pred).astype('float')\n",
    "        grad = np.where(residual > 0, -2 * alpha * residual, -2 * residual)\n",
    "        hess = np.where(residual > 0, 2 * alpha, 2.0)\n",
    "        return grad, hess\n",
    "    return weighted_mse_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_XGB(train, test, num, path='./', param=None, seed=42):\n",
    "    features, labels = train\n",
    "    y_train, y_valid, x_train, x_valid = train_test_split(features, labels, 24 * 7)\n",
    "    \n",
    "    if param is None:\n",
    "        print(f\"Setting Default Param\")\n",
    "        min_child_weight = 6\n",
    "        max_depth = 5\n",
    "        colsample_bytree = .8\n",
    "        subsample = .9\n",
    "    else:\n",
    "        assert len(param) == 6, f\"Param Invalid, param have to have 6 length but recieved param {len(param)} length\"\n",
    "        min_child_weight = param.min_child_weight\n",
    "        max_depth = int(param.max_depth)\n",
    "        colsample_bytree = param.colsample_bytree\n",
    "        subsample = param.subsample\n",
    "        \n",
    "    xgb_reg = XGBRegressor(\n",
    "        n_estimators=10000, eta=0.01,\n",
    "        min_child_weight=min_child_weight,\n",
    "        max_depth=max_depth,\n",
    "        colsample_bytree=colsample_bytree,\n",
    "        subsample=subsample,\n",
    "        seed=seed,\n",
    "        gpu_id=1,\n",
    "        tree_method='gpu_hist',\n",
    "        predictor='gpu_predictor'\n",
    "    )\n",
    "    xgb_reg.fit(\n",
    "        x_train, y_train, eval_set=[(x_train, y_train), (x_valid, y_valid)],\n",
    "        early_stopping_rounds=300,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    pred = xgb_reg.predict(x_valid)\n",
    "    pred = pd.Series(pred)\n",
    "    pred.index = np.arange(y_valid.index[0], y_valid.index[-1] + 1)\n",
    "    \n",
    "    # Test\n",
    "    # 건물 유형에 따른 예측 결과 확인\n",
    "    result = []\n",
    "    \n",
    "    for i in range(13, 25):\n",
    "        x_test_i = test[test[i] == 1]\n",
    "        y_pred_test = xgb_reg.predict(x_test_i)\n",
    "        test_series = pd.Series(\n",
    "            y_pred_test, \n",
    "            index=np.arange((y_valid.index.max() + 1), (y_valid.index.max() + 1 + len(y_pred_test)))\n",
    "        )\n",
    "        plot_series(y_train, y_valid, pred, test_series, markers=[',',',',',',','])\n",
    "        plt.title(f\"{num}_{i}\")\n",
    "        os.makedirs(os.path.join(path, 'results'), exist_ok=True)\n",
    "        plt.savefig(os.path.join(path, 'results', f\"{num}_{i}\"))\n",
    "        \n",
    "        smape_val = SMAPE(y_valid, pred)\n",
    "        print(f\"best iterations: {xgb_reg.best_iteration}\")        \n",
    "        print(f\"SMAPE: {smape_val}\")\n",
    "        result.append([smape_val, y_valid, pred, y_pred_test])\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting Default Param\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[15:34:29] /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/gbm/../common/common.h:239: XGBoost version not compiled with GPU support.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000176be59f8 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x0000000176ccb248 xgboost::gbm::GBTree::ConfigureUpdaters() + 476\n  [bt] (2) 3   libxgboost.dylib                    0x0000000176cc63a0 xgboost::gbm::GBTree::Configure(std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>>, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>>>> const&) + 952\n  [bt] (3) 4   libxgboost.dylib                    0x0000000176ce61e0 xgboost::LearnerConfiguration::Configure() + 1124\n  [bt] (4) 5   libxgboost.dylib                    0x0000000176bfe86c XGBoosterBoostedRounds + 104\n  [bt] (5) 6   libffi.8.dylib                      0x00000001031f804c ffi_call_SYSV + 76\n  [bt] (6) 7   libffi.8.dylib                      0x00000001031f5834 ffi_call_int + 1404\n  [bt] (7) 8   _ctypes.cpython-311-darwin.so       0x000000010345c140 _ctypes_callproc + 752\n  [bt] (8) 9   _ctypes.cpython-311-darwin.so       0x00000001034564a4 PyCFuncPtr_call + 228\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[39m=\u001b[39m fit_XGB((transformed, train_label), transformed_test, \u001b[39m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 29\u001b[0m, in \u001b[0;36mfit_XGB\u001b[0;34m(train, test, num, path, param, seed)\u001b[0m\n\u001b[1;32m     16\u001b[0m     subsample \u001b[39m=\u001b[39m param\u001b[39m.\u001b[39msubsample\n\u001b[1;32m     18\u001b[0m xgb_reg \u001b[39m=\u001b[39m XGBRegressor(\n\u001b[1;32m     19\u001b[0m     n_estimators\u001b[39m=\u001b[39m\u001b[39m10000\u001b[39m, eta\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m,\n\u001b[1;32m     20\u001b[0m     min_child_weight\u001b[39m=\u001b[39mmin_child_weight,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m     predictor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mgpu_predictor\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     28\u001b[0m )\n\u001b[0;32m---> 29\u001b[0m xgb_reg\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     30\u001b[0m     x_train, y_train, eval_set\u001b[39m=\u001b[39m[(x_train, y_train), (x_valid, y_valid)],\n\u001b[1;32m     31\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39m\u001b[39m300\u001b[39m,\n\u001b[1;32m     32\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     33\u001b[0m )\n\u001b[1;32m     35\u001b[0m pred \u001b[39m=\u001b[39m xgb_reg\u001b[39m.\u001b[39mpredict(x_valid)\n\u001b[1;32m     36\u001b[0m pred \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mSeries(pred)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m   1014\u001b[0m     obj \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1016\u001b[0m (\n\u001b[1;32m   1017\u001b[0m     model,\n\u001b[1;32m   1018\u001b[0m     metric,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[1;32m   1024\u001b[0m )\n\u001b[0;32m-> 1025\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_Booster \u001b[39m=\u001b[39m train(\n\u001b[1;32m   1026\u001b[0m     params,\n\u001b[1;32m   1027\u001b[0m     train_dmatrix,\n\u001b[1;32m   1028\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_num_boosting_rounds(),\n\u001b[1;32m   1029\u001b[0m     evals\u001b[39m=\u001b[39mevals,\n\u001b[1;32m   1030\u001b[0m     early_stopping_rounds\u001b[39m=\u001b[39mearly_stopping_rounds,\n\u001b[1;32m   1031\u001b[0m     evals_result\u001b[39m=\u001b[39mevals_result,\n\u001b[1;32m   1032\u001b[0m     obj\u001b[39m=\u001b[39mobj,\n\u001b[1;32m   1033\u001b[0m     custom_metric\u001b[39m=\u001b[39mmetric,\n\u001b[1;32m   1034\u001b[0m     verbose_eval\u001b[39m=\u001b[39mverbose,\n\u001b[1;32m   1035\u001b[0m     xgb_model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m   1036\u001b[0m     callbacks\u001b[39m=\u001b[39mcallbacks,\n\u001b[1;32m   1037\u001b[0m )\n\u001b[1;32m   1039\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[1;32m   1040\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[39mfor\u001b[39;00m k, arg \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(sig\u001b[39m.\u001b[39mparameters, args):\n\u001b[1;32m    619\u001b[0m     kwargs[k] \u001b[39m=\u001b[39m arg\n\u001b[0;32m--> 620\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/training.py:180\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    168\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(\n\u001b[1;32m    169\u001b[0m         EarlyStopping(rounds\u001b[39m=\u001b[39mearly_stopping_rounds, maximize\u001b[39m=\u001b[39mmaximize)\n\u001b[1;32m    170\u001b[0m     )\n\u001b[1;32m    171\u001b[0m cb_container \u001b[39m=\u001b[39m CallbackContainer(\n\u001b[1;32m    172\u001b[0m     callbacks,\n\u001b[1;32m    173\u001b[0m     metric\u001b[39m=\u001b[39mmetric_fn,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m     output_margin\u001b[39m=\u001b[39m\u001b[39mcallable\u001b[39m(obj) \u001b[39mor\u001b[39;00m metric_fn \u001b[39mis\u001b[39;00m feval,\n\u001b[1;32m    178\u001b[0m )\n\u001b[0;32m--> 180\u001b[0m bst \u001b[39m=\u001b[39m cb_container\u001b[39m.\u001b[39mbefore_training(bst)\n\u001b[1;32m    182\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_iteration, num_boost_round):\n\u001b[1;32m    183\u001b[0m     \u001b[39mif\u001b[39;00m cb_container\u001b[39m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/callback.py:148\u001b[0m, in \u001b[0;36mCallbackContainer.before_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''Function called before training.'''\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[0;32m--> 148\u001b[0m     model \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39mbefore_training(model\u001b[39m=\u001b[39mmodel)\n\u001b[1;32m    149\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mbefore_training should return the model\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    150\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_cv:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/callback.py:350\u001b[0m, in \u001b[0;36mEarlyStopping.before_training\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbefore_training\u001b[39m(\u001b[39mself\u001b[39m, model: _Model) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m _Model:\n\u001b[0;32m--> 350\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarting_round \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mnum_boosted_rounds()\n\u001b[1;32m    351\u001b[0m     \u001b[39mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:2469\u001b[0m, in \u001b[0;36mBooster.num_boosted_rounds\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2467\u001b[0m rounds \u001b[39m=\u001b[39m ctypes\u001b[39m.\u001b[39mc_int()\n\u001b[1;32m   2468\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2469\u001b[0m _check_call(_LIB\u001b[39m.\u001b[39mXGBoosterBoostedRounds(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle, ctypes\u001b[39m.\u001b[39mbyref(rounds)))\n\u001b[1;32m   2470\u001b[0m \u001b[39mreturn\u001b[39;00m rounds\u001b[39m.\u001b[39mvalue\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/xgboost/core.py:279\u001b[0m, in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \n\u001b[1;32m    270\u001b[0m \u001b[39mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39m    return value from API calls\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    278\u001b[0m \u001b[39mif\u001b[39;00m ret \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 279\u001b[0m     \u001b[39mraise\u001b[39;00m XGBoostError(py_str(_LIB\u001b[39m.\u001b[39mXGBGetLastError()))\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [15:34:29] /Users/runner/work/xgboost/xgboost/python-package/build/temp.macosx-11.0-arm64-cpython-38/xgboost/src/gbm/../common/common.h:239: XGBoost version not compiled with GPU support.\nStack trace:\n  [bt] (0) 1   libxgboost.dylib                    0x0000000176be59f8 dmlc::LogMessageFatal::~LogMessageFatal() + 124\n  [bt] (1) 2   libxgboost.dylib                    0x0000000176ccb248 xgboost::gbm::GBTree::ConfigureUpdaters() + 476\n  [bt] (2) 3   libxgboost.dylib                    0x0000000176cc63a0 xgboost::gbm::GBTree::Configure(std::__1::vector<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>>, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char>>>>> const&) + 952\n  [bt] (3) 4   libxgboost.dylib                    0x0000000176ce61e0 xgboost::LearnerConfiguration::Configure() + 1124\n  [bt] (4) 5   libxgboost.dylib                    0x0000000176bfe86c XGBoosterBoostedRounds + 104\n  [bt] (5) 6   libffi.8.dylib                      0x00000001031f804c ffi_call_SYSV + 76\n  [bt] (6) 7   libffi.8.dylib                      0x00000001031f5834 ffi_call_int + 1404\n  [bt] (7) 8   _ctypes.cpython-311-darwin.so       0x000000010345c140 _ctypes_callproc + 752\n  [bt] (8) 9   _ctypes.cpython-311-darwin.so       0x00000001034564a4 PyCFuncPtr_call + 228\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result = fit_XGB((transformed, train_label), transformed_test, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
